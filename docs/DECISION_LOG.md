# ***Content and structure until line 102 must be rewritten to reflect desicions!!***
# ***Valid data from line 102***

# Process and findings of the onnx-acoustic project
*Documentation*

1. ## Local foundation with placeholder model

   1. ### create `DummyAcousticModel` based on `nn.Module` 
    - initialize instance - requires output dimensions
        - creates linear neural network layer (`nn.Module.Linear`), which projects input dimensions to output dimensions
            - forward function defines pooled value which will be a mean of the input, which will then be used as an input for the nn layer; the result is stored in and returned as `output_vector`
   
    - create `export_dummy_onnx` function which requires `model_path` (has a default pointing to the default output path/name)
        - create an `DummyAcousticModel` instance with `output_dim==ACOUSTIC_VECTOR_DIM` called `model`
        - create `dummy_input` which is a random tensor with `1` row, `MOCK_INPUT_LENGTH` columns (16000*5) and `float32` datatype and will be exported as input for the model
        - strings to give input and output nodes of the model a recoginzable name are defined and also passed to the export function
        - the `torch.onnx.export` function is called and defines the export format of the placeholder model:
            - the model instance
            - the model input
            - its file path
            - if weights should be exported (`True`)
            - how `input_names` and `out_names` for nodes are defined
            - tell exporter that second axis (axis `1`) of the input tensor (`input_raw_audio`) is dynamic and name it `time_steps`; this allows the model to accept inputs of different lengths
        
    - The model_path is printed to the console along with the input shape requirements `(1,N)` and output expectations `(1, 512)` (`ACOUSTIC_VECTOR_DIM`)
    
    - if the script is run as `"__main__"`, the `export_dummy_onnx` function is called  

   2. ### Run model locally in Python environment with onnx runtime
     When the `acoustic_processor.py` script is called.
     The `test_processor` function is called with `DUMMY_MODEL` and `SAMPLE_AUDIO` as args.
     1. It creates a model processing harness called `processor` (an Instance of `AcousticModelProcessor`)
       - `AcousticModelProcessor` provides the inference session, an `input_name` and `output_name`
       - `AcousticModelProcessor` provides a `generate_vector` function which will allow its instances to take in `preprocessed_audio` and return the result (later stored as the resulting `acoustic_vector`)
     2. `preprocessed_audio` is generated by loading the `audio_file_path` as `audio_path` into the `load_and_preprocess_audio` function
       - audio file is loaded with `librosa` resulting in an audio time series and a sample rate which we predefine with the `TARGET_SR` constant
       - audio time series shape is expanded by 1 dimension to make it processable for the onnx runtime; was `(N,)`, now is `(1, N)`
       - resulting `audio_data_batched` object is stored as `preprocessed_audio`
     3. `acoustic_vector` is the result of running the .generate_vector method of the processor instance with the `preprocessed_audio` as function input
       - running the inference: model makes predictions against a preprocessed input
       - details of `acoustic_vector` are printed to the console (shape, dtype and first 5 elements)
       - in case of an error, error code and message are printed to terminal.


   3. ### Create CLI app and monitor performance metrics (time, memory)
      1. Set up
        - along with `import typer`(), `import time`(), `import json`(), `import numpy as np`(), `from typing import Annotated` and `from memory_profiler import profile`, we also import `from src.acoustic_processor import AcousticModelProcessor, load_and_preprocess_audio` which are our model harness and the function to preprocess the input to prepare it for the onnx model
        - create a `typer.Typer()` instance called `app` and define the constant `MODEL_PATH`
        - `try:` to create `processor` as an instance of(and argument) `AcousticModelProcessor(MODEL_PATH)`
        - `except`… the attempt fails, the error message and code are printed to the terminal
      2. Test run, run and profiling
        - `@profile`
        - define `profile_inference` function with `processor` and `preprocessed_audio` parameters
        - warm up run
        - `start time`r, run and store as `vector`, `end time`r
        - calculate `processing_time_ms`
        - `return vector, processing_time_ms`
      3. create `@app.command()`
        - define `main` (because single command CLI app) with parameters `audio_path`, `output_json` and `help` / update user on vector generation and file
        - `try` to 
          - get the `preprocessed_audio` with the provided `audio_path` argument
          - run the `profile_inference` function with `processor` and `preprocessed_audio` as arguments to get `vector` and `processing_time_ms`
          - create an object called `output_data` with all outputs (source file, number of samples from shape of input (here 66000), measured time, vector shape as list, vector with 512 dimensional values)
          - save the results / write `output_json`
          - `typer.echo` the results to the terminal (time, vector shape and file name/path)
        - `except` an error occurs
          - then echo a message and the error code to the user
        - if the script is called as `"__main__"`, the `app()`is called 

   4. ### Documentation / Tidy Up

      #### Retrospective
      - "What did you learn that wasn't in the SAA-C03 exam?"
        - Python, especially PyTorch's nn.Module
        - onnx runtime
        - reading documentation and checking it against suggested code
        - reading and retelling code until I understand it
      - "What would you do differently if you started over?"
        - I would have a better git flow and cleaner documentation process but these are things I learned during this phase
      - "If a hiring manager asked about this, what's your 2-minute explanation?"
        - I created and deployed a basic neural network to understand its endpoints and how I would measure its performance
        - by understanding the endpoints, their requirements and the compute requirements of the placeholder model we can make first decisions on a naive EC2 deployment in phase2
      - "What are you going to put in your decision log about this?"
        - I decided to _not_ use CoPilot in VSCode as it kept me from focussing what I actually wanted to write -> I can use it when I better judge what it suggests
        

      #### Findings / gathered metrics

          | Category | Metric | Value (Your Results) | Notes for Portfolio |
          | :--- | :--- | :--- | :--- |
          | **I/O: Input Signal** | Format | NumPy `float32` | Standard for ML models. |
          | **I/O: Input Shape** | Shape | `(1, N)` | Dynamic length (`N`), 1 batch. |
          | **I/O: Sample Rate** | Target SR | 16,000 Hz | Required pre-processing step. |
          | **I/O: Output Vector** | Shape | `(1, 512)` | Fixed size signature of the acoustics. |
          | **Performance: Load Time** | Model Init Time | (TBD, but happens once) | Time to initialize `AcousticModelProcessor` |
          | **Performance: Inference** | Processing Time | **~0.050 ms** | Exceptionally fast for a 4-second audio clip. |
          | **Performance: Memory** | Peak RAM (Inference) | **~210 MiB** | Note that the model *loading* takes most of this RAM. |


      #### Merging feature/fastapi-wrapper -> main

      #### Cleaning .gitignore
   
2. ## Phase 2: Naive EC2 Deployment - Initial Instance Configuration

**Date:** 2025-10-26 – 2025-10-27

### 1. Decision: EC2 Architecture and Instance Type

*   **Choice:**
    * **AMI:** Amazon-Linux 2023 kernel-6.1 AMI (x86_64 architecture, suggested by AWS)
    * **Instance Type:** `t2.micro` (1vCPU, 1GiB RAM memory)
*   **Reasoning:**
    * My primary constraint here was to move with caution and try to build with free tier resources while meeting the compute requirements.
    From the local deployment I learned that the app was using  > 210 MiB in working memory which would make 0.5 GiB RAM (t2.nano) quite tight and so I chose the `t2.micro` with 1GiB RAM is within the free tier, providing a robust buffer for the OS and application runtime.
    * I chose the Amazon-Linux 2023 kernel-6.1 AMI because it's an up-to-date AWS supported default, which ensures up-to-date packages and security patches.
*   **Alternaitves considered:** 
    * If this free tier instance doesn't meet the compute requirements, or when the free tier ends, my first alternative choice would be a `t4g.micro`, so a Graviton/arm64 architecture, as it offers the best price performance and is the more modern hardware and therefor more sustainable to learn and build on.

### 2. Decision: Network Security (Security Groups)

*   **Choice: A security group with 2 rules**
    * **Rule 1: SSH Access for instance management:** 
    * To keep the setup as basic as possible, for remote access of the instance OS, we will set up a Security Group allowing access to the instance's network interface via SSH protocol on port22, restricted to my IP address.
    * **Rule 2: Public HTTP access:** 
    * Because Security Groups are Layer 4 FWs TCP is defined as the protocol as it transports HTTP. So we allow all traffic on port 8000 like its defined in our app, `0.0.0.0:8000`. In a later stage an ALB will handle the SSL certificat handling and provide HTTPS security.
*   **Alternatives/Rejected:**
    * Using a `0.0.0.0/0` rule (SGs always allow/never deny) for both SGs and on both ports, was considered and rejected.

### 3. Decision: Instance Access

*   **Choice:** SSH keys / EC2 key pair
    * I chose the key pair as the most basic means of accessing the instance and experiencing the fundamentals. I'll download a `.pem` file at instance launch and use it to connect via my local SSH client.
*   **Alternatives/Rejected:**
    * Though being used to EC2 instance connect from Adrian Cantrill's SAA C03 course, I rejected the method, to keep the deployment *naive*.




